import json
from pathlib import Path
from typing import Literal
from pydantic import BaseModel
from openai import OpenAI
from dotenv import load_dotenv
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock


load_dotenv(os.path.join(os.path.dirname(__file__), "keys.env"))
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

MAX_WORKERS = 48
KEY_POINT_DIR = os.path.join(os.path.dirname(__file__), "key_point")

# Thread lock for OpenAI client (to be safe with rate limits)
client_lock = Lock()


def create_prompt(key_point, answer):
    return f"""You are given a **single key point** and a **report**.

    Your job is to determine whether the report:
    - **Supports** the key point (it affirms, explains, or reinforces the point),
    - **Omits** the key point (it does not mention or cover this point at all), or
    - **Contradicts** the key point (it says something that disagrees with or negates the point).

    Carefully read the key point and the report.

    Return your answer as a **JSON object** with two fields:
    - "label": One of "Supported", "Omitted", or "Contradicted".
    - "justification": Brief explanation on why you assigned this label.

    Respond strictly in JSON format:
    {{"label": label, "justification": justification}}
    Do **not** add any extra commentary or text outside the JSON.

    ---

    Key Point: {key_point}
    Report: {answer}
    """


class KeyPointRecall(BaseModel):
    label: Literal["Supported", "Omitted", "Contradicted"]
    justification: str


def evaluate_single_key_point(key_point, answer):
    """
    Evaluate a single key point against the answer using OpenAI API.
    
    Args:
        key_point: Dictionary containing key point information
        answer: The answer text to evaluate
        
    Returns:
        Tuple of (point_number, (label, justification))
    """
    prompt = create_prompt(key_point["point_content"], answer)
    chat_pattern = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ]
    
    # Use thread lock to prevent potential rate limit issues
    with client_lock:
        response = client.beta.chat.completions.parse(
            model="gpt-5-nano-2025-08-07",
            messages=chat_pattern,
            response_format=KeyPointRecall,
        )
    
    result = json.loads(response.choices[0].message.content)
    return key_point["point_number"], (result['label'], result['justification'])


def evaluate_answer(answer, key_points):
    """
    Evaluate all key points against the answer using multi-threading.
    
    Args:
        answer: The answer text to evaluate
        key_points: List of key point dictionaries
        
    Returns:
        Dictionary mapping point_number to (label, justification)
    """
    results = {}
    
    # Use ThreadPoolExecutor for concurrent API calls
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit all tasks
        future_to_point = {
            executor.submit(evaluate_single_key_point, key_point, answer): key_point
            for key_point in key_points
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_point):
            try:
                point_number, (label, justification) = future.result()
                results[point_number] = (label, justification)
            except Exception as e:
                key_point = future_to_point[future]
                print(f"Error evaluating key point {key_point.get('point_number', 'unknown')}: {e}")
                # Set default values for failed evaluations
                results[key_point.get('point_number', 'unknown')] = ("Omitted", "Error in evaluation")
    
    return results


def evaluate_query_kpr(query_id, answer):
    """
    Evaluate key point recall for a single query.
    
    Args:
        query_id: Query identifier
        answer: Answer text to evaluate
    
    Returns:
        Dictionary containing evaluation results and statistics
    """
    key_point_dir = Path(KEY_POINT_DIR)
    p_path = key_point_dir / f"{query_id}_aggregated.json"

    if not p_path.exists():
        raise FileNotFoundError(f"Missing key point file for query {query_id}: {p_path}")

    # Load key points
    with open(p_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        key_points = data["key_points"]

    # Evaluate answer using multi-threading
    evaluations = evaluate_answer(answer, key_points)
    
    # Calculate statistics
    supported_count = 0
    omitted_count = 0
    contradicted_count = 0
    
    for point_number, (label, justification) in evaluations.items():
        if label == "Supported":
            supported_count += 1
        elif label == "Omitted":
            omitted_count += 1
        elif label == "Contradicted":
            contradicted_count += 1
    
    total_points = len(evaluations)
    support_rate = supported_count / total_points if total_points > 0 else 0
    omitted_rate = omitted_count / total_points if total_points > 0 else 0
    contradicted_rate = contradicted_count / total_points if total_points > 0 else 0
    
    return {
        "support_rate": support_rate,
        "omitted_rate": omitted_rate,
        "contradicted_rate": contradicted_rate
    }


