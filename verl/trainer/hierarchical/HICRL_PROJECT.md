# Hierarchical In-Context Reinforcement Learning (HICRL)

This project implements a two-level Reinforcement Learning framework designed to improve the reasoning capabilities of Language Models by explicitly training them to generate and utilize high-quality in-context demonstrations.

## 1. Core Architecture

The system consists of two primary components acting in a hierarchical fashion:

### Higher-Level (HL): Context Generator $\pi_\theta(c \mid x)$
*   **Role**: Given an input problem $x$, the HL model generates a synthetic demonstration $c = (x', y')$.
*   **Objective**: Learn to produce contexts that maximize the Lower-Level model's performance and confidence.
*   **Training**: Optimized via RLHF where the reward is a combination of the LL's task success and the LL's predictive entropy (confidence).

### Lower-Level (LL): Task Solver $p_\phi(y \mid x, c)$
*   **Role**: Solves the task $x$ conditioned on the context $c$ generated by the HL.
*   **Objective**: Effectively utilize provided demonstrations to arrive at the correct reasoning trace and answer $y$.
*   **Training**: Optimized via GRPO (Group Relative Policy Optimization).

---

## 2. Key Mathematical Objectives

### Standard GRPO Backbone
The LL is trained using a critic-less GRPO objective, where advantages are computed relative to the mean reward of a sampled group:
\[ \max_\phi \mathbb{E} \left[ \sum_i A_i \log p_\phi(y_i \mid x, c_i) \right] - \beta_{\text{LL}} D_{\text{KL}}(p_\phi \| p_{\text{ref}}) \]

### Forcing Context Usage (KL-Maximization)
To prevent the LL from ignoring the context (emergent ICL vs. forced dependency), we introduce a **Context Utilization Term**:
\[ U_\phi(x,c) = D_{\text{KL}}(p_\phi(\cdot \mid x,c) \| q(\cdot \mid x)) \]
where $q(y \mid x)$ is a context-free prior (frozen initial LL). This forces the LL to diverge from its context-free behavior, making the context functionally necessary for the final generation.

### Reward Shaping
The final reward used for LL training is:
\[ r_i^{\text{total}} = R_{\text{task}}(x,y_i) + \lambda \cdot \text{clip}(\log p_\phi(y_i \mid x,c_i) - \log q(y_i \mid x), \tau') \]

---

## 3. Implementation Details in `verl-agent`

### `HierarchicalContextManager`
*   Orchestrates the generation of contexts using a standalone HuggingFace model.
*   Enforces a strict **2048 token limit** for both training and testing.
*   Dynamically augments the `DataProto` batches during the RL rollout loop.

### `HierarchicalLogger`
Provides granular logging to answer: *"Is the model actually using the context?"*
*   Logs are saved per-question: `logs/{model}/{benchmark}/{split}/sample_{id}.jsonl`.
*   Records `logp_ctx`, `logp_noctx`, and `delta_logp` to track utilization strength.

### Semantic Reward (LLM-as-Judge)
*   Uses **GPT-4o-mini via CMU Gateway** as the primary evaluator.
*   Determines semantic equivalence between the predicted answer and ground truth, moving beyond fragile string matching.
*   Includes a robust rule-based fallback for math expressions.

---

## 4. Experimental Phases

1.  **Phase 0-1**: Establishing GRPO baselines and testing the impact of static/pretrained contexts (Exp 1, 2, 4).
2.  **Phase 2**: Enabling KL-Maximization to force the model to rely on generated demonstrations (Exp 6, 7).
3.  **Phase 3**: Alternating optimization where the HL and LL are updated iteratively to form a self-improving loop.
4.  **Phase 4**: Stability and stress tests (random contexts, multiple samples, etc.).

