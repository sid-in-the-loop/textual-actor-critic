wandb: WARNING `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.
wandb: Currently logged in as: ssmurali (ssmurali-cmu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
+++ hostname
+++ whoami
++ MACHINE_SPECIFIC_RAY_DIR=/tmp/ray_babel-s5-32_ssmurali_1998814
++ mkdir -p /tmp/ray_babel-s5-32_ssmurali_1998814
++ export RAY_TMPDIR=/tmp/ray_babel-s5-32_ssmurali_1998814
++ RAY_TMPDIR=/tmp/ray_babel-s5-32_ssmurali_1998814
+++ nvidia-smi --query-gpu=name --format=csv,noheader
+++ head -n 1
++ GPU_MODEL='NVIDIA L40S'
++ [[ NVIDIA L40S == *\A\6\0\0\0* ]]
++ [[ NVIDIA L40S == *\L\4\0\S* ]]
++ echo 'Detected NVIDIA L40S, disabling NCCL P2P'
++ export NCCL_P2P_DISABLE=1
++ NCCL_P2P_DISABLE=1
++ export VLLM_ATTENTION_BACKEND=XFORMERS
++ VLLM_ATTENTION_BACKEND=XFORMERS
++ export HYDRA_FULL_ERROR=1
++ HYDRA_FULL_ERROR=1
++ export RAY_DEBUG=0
++ RAY_DEBUG=0
++ export WANDB_API_KEY=1e255990efc627595f0c805e0546cc7f0ff08b17
++ WANDB_API_KEY=1e255990efc627595f0c805e0546cc7f0ff08b17
++ export HF_TOKEN=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
++ HF_TOKEN=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
++ export HUGGING_FACE_HUB_TOKEN=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
++ HUGGING_FACE_HUB_TOKEN=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
++ export OPENAI_API_KEY=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
++ OPENAI_API_KEY=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
++ export OPENAI_API_KEY=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
++ OPENAI_API_KEY=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
++ MODEL_DIR=/data/group_data/cx_group/verl_agent_shared
++ train_data_size=32
++ val_data_size=16
++ group_size=8
++ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo algorithm.use_kl_in_reward=False algorithm.compute_mean_std_cross_all_data=False data.train_files=dummy_data/text/train.parquet data.val_files=dummy_data/text/val.parquet data.train_batch_size=32 data.val_batch_size=16 data.max_prompt_length=20000 data.max_response_length=1024 data.filter_overlong_prompts=True data.truncation=error data.return_raw_chat=True actor_rollout_ref.rollout.temperature=1.0 actor_rollout_ref.model.path=/data/group_data/cx_group/behavior_priming/checkpoint/qwen3_1.7b/web_qwen_sft_behavior/checkpoint-924 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.actor.use_dynamic_bsz=True actor_rollout_ref.actor.ppo_max_token_len_per_gpu=22000 actor_rollout_ref.actor.ppo_mini_batch_size=32 actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.actor.ulysses_sequence_parallel_size=1 actor_rollout_ref.model.enable_activation_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=64 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.5 actor_rollout_ref.rollout.enable_chunked_prefill=False actor_rollout_ref.rollout.enforce_eager=True actor_rollout_ref.rollout.free_cache_engine=True actor_rollout_ref.rollout.val_kwargs.temperature=0.4 actor_rollout_ref.rollout.val_kwargs.do_sample=True actor_rollout_ref.rollout.max_num_batched_tokens=22000 actor_rollout_ref.rollout.max_model_len=22000 actor_rollout_ref.rollout.disable_log_stats=False actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=64 actor_rollout_ref.ref.fsdp_config.param_offload=True actor_rollout_ref.ref.max_model_len=22000 actor_rollout_ref.actor.use_invalid_action_penalty=True env.rule_reward_coef=0 env.env_name=deepresearch env.dataset=afm env.seed=0 env.rollout.n=8 env.rollout.k=1 env.max_steps=5 env.use_explicit_thinking=False env.use_critique=False env.replace_input=False env.use_rule_reward=False env.rule_number=5 env.use_dense_reward=False env.belief_shaped_grpo.enable=True env.belief_shaped_grpo.alpha=1.0 env.belief_shaped_grpo.max_candidates=3 trainer.critic_warmup=0 'trainer.logger=[console,wandb]' trainer.project_name=DeepResearch_RL trainer.experiment_name=deepresearch_1.7b_belief_shaped_grpo trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=5 trainer.test_freq=25 trainer.total_epochs=1 trainer.total_training_steps=50 trainer.resume_mode=disable trainer.default_local_dir=/data/group_data/cx_group/verl_agent_shared/checkpoint/deepresearch_1.7b_belief_shaped_grpo trainer.val_before_train=False trainer.log_val_generations=1
2025-12-20 18:40:24,517	ERROR services.py:1357 -- Failed to start the dashboard 
2025-12-20 18:40:24,518	ERROR services.py:1382 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.
2025-12-20 18:40:24,518	ERROR services.py:1426 -- 
The last 20 lines of /tmp/ray_babel-s5-32_ssmurali_1998814/ray/session_2025-12-20_18-40-04_107547_1998822/logs/dashboard.log (it contains the error message from the dashboard): 
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/multiprocessing/connection.py", line 382, in _recv
    raise EOFError
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/ray/dashboard/dashboard.py", line 297, in <module>
    loop.run_until_complete(dashboard.run())
  File "/home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/asyncio/base_events.py", line 664, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/ray/dashboard/dashboard.py", line 98, in run
    await self.dashboard_head.run()
  File "/home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/ray/dashboard/head.py", line 405, in run
    handle.wait_for_module_ready()
  File "/home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/ray/dashboard/subprocesses/handle.py", line 146, in wait_for_module_ready
    raise RuntimeError(
RuntimeError: Module MetricsHead failed to start. Received EOF from pipe.
2025-12-20 18:40:24,720	INFO worker.py:1951 -- Started a local Ray instance.
[2025-12-20 18:40:24,722 I 1998822 1998822] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1
[33m(raylet)[0m [2025-12-20 18:40:25,326 I 1999933 1999933] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1
[36m(TaskRunner pid=2003379)[0m Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
[36m(TaskRunner pid=2003379)[0m Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
[36m(TaskRunner pid=2003379)[0m Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.
[36m(TaskRunner pid=2003379)[0m See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
[33m(raylet)[0m [2025-12-20 18:40:25,892 I 2003379 2003379] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(TaskRunner pid=2003379)[0m building DeepResearchMultiProcessEnv with config: {'verbose': False, 'log_dir': '/home/ssmurali/verl-agent/agent_system/environments/env_package/deepresearch/../../../../deepresearch_logs_Belief/afm_train/20251220_184032', 'answer_dir': '/home/ssmurali/verl-agent/agent_system/environments/env_package/deepresearch/../../../../deepresearch_outputs_Belief/afm_train/20251220_184032', 'max_turns': 5, 'num_docs': 1, 'num_docs_to_read': 1, 'search_reminder_turn': 3, 'final_report_reminder_turn': 2, 'max_context_length': 8000, 'mode': 'qa', 'search_engine': 'serper', 'use_explicit_thinking': False, 'use_critique': False}
[36m(TaskRunner pid=2003379)[0m Init DeepResearchMultiProcessEnv, env_num: 32, group_n: 8, num_processes: 256
[36m(TaskRunner pid=2003379)[0m building DeepResearchMultiProcessEnv with config: {'verbose': False, 'log_dir': '/home/ssmurali/verl-agent/agent_system/environments/env_package/deepresearch/../../../../deepresearch_logs_Belief/afm_val/20251220_184032', 'answer_dir': '/home/ssmurali/verl-agent/agent_system/environments/env_package/deepresearch/../../../../deepresearch_outputs_Belief/afm_val/20251220_184032', 'max_turns': 5, 'num_docs': 1, 'num_docs_to_read': 1, 'search_reminder_turn': 3, 'final_report_reminder_turn': 2, 'max_context_length': 8000, 'mode': 'qa', 'search_engine': 'serper', 'use_explicit_thinking': False, 'use_critique': False}
[36m(TaskRunner pid=2003379)[0m Init DeepResearchMultiProcessEnv, env_num: 16, group_n: 1, num_processes: 16
[36m(pid=2003558)[0m Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.[32m [repeated 3x across cluster][0m
[36m(pid=2003558)[0m Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.[32m [repeated 3x across cluster][0m
[36m(pid=2003558)[0m Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.[32m [repeated 3x across cluster][0m
[36m(pid=2003558)[0m See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.[32m [repeated 3x across cluster][0m
[33m(raylet)[0m [2025-12-20 18:40:37,344 I 2011556 2011556] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1[32m [repeated 186x across cluster][0m
[36m(pid=2004661)[0m Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.[32m [repeated 67x across cluster][0m
[36m(pid=2004661)[0m Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.[32m [repeated 67x across cluster][0m
[36m(pid=2004661)[0m Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.[32m [repeated 67x across cluster][0m
[36m(pid=2004661)[0m See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.[32m [repeated 67x across cluster][0m
[33m(raylet)[0m [2025-12-20 18:40:39,235 I 2016913 2016913] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1[32m [repeated 86x across cluster][0m
[36m(pid=2005294)[0m Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.[32m [repeated 102x across cluster][0m
[36m(pid=2005294)[0m Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.[32m [repeated 102x across cluster][0m
[36m(pid=2005294)[0m Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.[32m [repeated 102x across cluster][0m
[36m(pid=2005294)[0m See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.[32m [repeated 102x across cluster][0m
[36m(TaskRunner pid=2003379)[0m Filtering prompts longer than 20000 tokens:   0%|          | 0/1600 [00:00<?, ? examples/s]
[36m(TaskRunner pid=2003379)[0m Filtering prompts longer than 20000 tokens:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [00:01<00:00, 776.44 examples/s]
[36m(TaskRunner pid=2003379)[0m Filtering prompts longer than 20000 tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:02<00:00, 604.78 examples/s]Filtering prompts longer than 20000 tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:02<00:00, 628.27 examples/s]
[36m(TaskRunner pid=2003379)[0m Filtering prompts longer than 20000 tokens:   0%|          | 0/512 [00:00<?, ? examples/s]
[36m(TaskRunner pid=2003379)[0m Filtering prompts longer than 20000 tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:00<00:00, 1880.58 examples/s]
[36m(TaskRunner pid=2003379)[0m Filtering prompts longer than 20000 tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:00<00:00, 1840.86 examples/s]
[36m(TaskRunner pid=2003379)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(pid=2016178)[0m Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.[32m [repeated 98x across cluster][0m
[36m(pid=2016178)[0m Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.[32m [repeated 98x across cluster][0m
[36m(pid=2016178)[0m Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.[32m [repeated 98x across cluster][0m
[36m(pid=2016178)[0m See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.[32m [repeated 98x across cluster][0m
[33m(raylet)[0m [2025-12-20 18:40:53,352 I 2021716 2021716] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1
[36m(TaskRunner pid=2003379)[0m WARNING:2025-12-20 18:40:53,873:Waiting for register center actor fWN7jx_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(pid=2010592)[0m Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.[32m [repeated 2x across cluster][0m
[36m(pid=2010592)[0m Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.[32m [repeated 2x across cluster][0m
[36m(pid=2010592)[0m Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.[32m [repeated 2x across cluster][0m
[36m(pid=2010592)[0m See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.[32m [repeated 2x across cluster][0m
[33m(raylet)[0m [2025-12-20 18:41:11,902 I 2025733 2025733] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2025908)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[33m(raylet)[0m [2025-12-20 18:41:12,579 I 2025910 2025910] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2021863)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2021863)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2021863)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=2021863)[0m /home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2021863)[0m   warnings.warn(
[36m(WorkerDict pid=2025910)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=2025910)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=2003379)[0m wandb: WARNING `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.
[36m(TaskRunner pid=2003379)[0m wandb: Currently logged in as: ssmurali (ssmurali-cmu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=2003379)[0m wandb: Tracking run with wandb version 0.21.3
[36m(TaskRunner pid=2003379)[0m wandb: Run data is saved locally in /home/ssmurali/verl-agent/wandb/run-20251220_184139-hhpy5ef7
[36m(TaskRunner pid=2003379)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=2003379)[0m wandb: Syncing run deepresearch_1.7b_belief_shaped_grpo
[36m(TaskRunner pid=2003379)[0m wandb: â­ï¸ View project at https://wandb.ai/ssmurali-cmu/DeepResearch_RL
[36m(TaskRunner pid=2003379)[0m wandb: ðŸš€ View run at https://wandb.ai/ssmurali-cmu/DeepResearch_RL/runs/hhpy5ef7
[36m(TaskRunner pid=2003379)[0m Training Progress:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(WorkerDict pid=2025910)[0m /home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2025910)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m /home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.
[36m(TaskRunner pid=2003379)[0m   return _methods._mean(a, axis=axis, dtype=dtype,
[36m(TaskRunner pid=2003379)[0m /home/ssmurali/miniconda3/envs/verl-agent/lib/python3.12/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide
[36m(TaskRunner pid=2003379)[0m   ret = ret.dtype.type(ret / rcount)
[36m(TaskRunner pid=2003379)[0m Training Progress:   2%|â–         | 1/50 [46:26<37:55:17, 2786.07s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:   4%|â–         | 2/50 [1:18:45<30:30:14, 2287.80s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:   6%|â–Œ         | 3/50 [1:56:45<29:49:34, 2284.57s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:   8%|â–Š         | 4/50 [2:39:33<30:37:18, 2396.50s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  10%|â–ˆ         | 5/50 [3:12:00<27:55:42, 2234.28s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  12%|â–ˆâ–        | 6/50 [3:46:04<26:30:54, 2169.43s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  14%|â–ˆâ–        | 7/50 [4:22:29<25:58:26, 2174.57s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  16%|â–ˆâ–Œ        | 8/50 [4:55:19<24:36:33, 2109.37s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  18%|â–ˆâ–Š        | 9/50 [5:34:00<24:46:42, 2175.67s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  20%|â–ˆâ–ˆ        | 10/50 [6:12:14<24:34:44, 2212.12s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 11/50 [6:50:12<24:10:57, 2232.24s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 12/50 [7:24:21<22:58:27, 2176.51s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [8:03:15<22:51:37, 2224.27s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 14/50 [8:37:59<21:49:13, 2182.04s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [9:17:27<21:45:28, 2237.97s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [9:57:59<21:41:19, 2296.47s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [10:30:18<20:03:56, 2188.98s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [11:06:03<19:20:22, 2175.69s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [11:33:51<17:25:16, 2023.10s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [12:07:44<16:53:06, 2026.20s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [12:45:11<16:51:23, 2092.55s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [13:20:01<16:16:09, 2091.77s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [14:04:04<16:55:40, 2257.04s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [14:38:55<15:56:31, 2207.38s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m 
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Reset 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m Step 16 workers (env_num=16, group_n=1, dataset_name=afm_val)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 25 that is less than the current step 57. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [17:41:47<33:35:24, 4836.99s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [18:15:48<26:39:14, 3998.11s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 26 that is less than the current step 58. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [18:49:17<21:43:49, 3401.30s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 27 that is less than the current step 59. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [19:25:00<18:28:46, 3023.92s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 28 that is less than the current step 60. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [20:12:07<17:17:42, 2964.88s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 29 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [20:54:21<15:45:12, 2835.60s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 30 that is less than the current step 62. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [21:32:34<14:06:22, 2672.74s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 31 that is less than the current step 63. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [22:04:23<12:13:04, 2443.58s/it]Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 32 that is less than the current step 64. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [22:34:39<10:39:01, 2255.37s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 33 that is less than the current step 65. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m 
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [23:11:31<9:57:58, 2242.43s/it] 
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 34 that is less than the current step 66. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Training Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [23:45:29<9:05:14, 2180.96s/it]
[36m(TaskRunner pid=2003379)[0m Reset 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m wandb: WARNING Tried to log to step 35 that is less than the current step 67. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
[36m(TaskRunner pid=2003379)[0m Step 256 workers (env_num=32, group_n=8, dataset_name=afm_train)
slurmstepd: error: *** JOB 5942361 ON babel-s5-32 CANCELLED AT 2025-12-21T18:40:09 DUE TO TIME LIMIT ***
*** SIGTERM received at time=1766360409 on cpu 40 ***
PC: @     0x153d0148688a  (unknown)  __futex_abstimed_wait_common
    @     0x153d0143e730  (unknown)  (unknown)
[2025-12-21 18:40:09,045 E 1998822 1998822] logging.cc:474: *** SIGTERM received at time=1766360409 on cpu 40 ***
[2025-12-21 18:40:09,045 E 1998822 1998822] logging.cc:474: PC: @     0x153d0148688a  (unknown)  __futex_abstimed_wait_common
[2025-12-21 18:40:09,046 E 1998822 1998822] logging.cc:474:     @     0x153d0143e730  (unknown)  (unknown)
